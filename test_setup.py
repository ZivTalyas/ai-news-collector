#!/usr/bin/env python3
"""
Test Setup Script for AI News Collector
Verifies that all components are working correctly
"""

import sys
import os
from dotenv import load_dotenv

def test_imports():
    """Test that all required packages can be imported"""
    print("ğŸ” Testing Python imports...")
    
    try:
        import requests
        import beautifulsoup4
        import pymongo
        import streamlit
        import googlesearch
        import pandas
        print("âœ… All required packages imported successfully")
        return True
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print("ğŸ’¡ Run: pip install -r requirements.txt")
        return False

def test_environment():
    """Test environment variables"""
    print("\nğŸ” Testing environment variables...")
    
    load_dotenv()
    mongo_uri = os.getenv('MONGO_URI')
    
    if not mongo_uri:
        print("âŒ MONGO_URI not found in environment variables")
        print("ğŸ’¡ Copy env_template.txt to .env and add your MongoDB connection string")
        return False
    
    if "mongodb+srv://" not in mongo_uri:
        print("âš ï¸  MONGO_URI doesn't look like a MongoDB Atlas connection string")
        return False
    
    print("âœ… Environment variables configured")
    return True

def test_database_connection():
    """Test MongoDB connection"""
    print("\nğŸ” Testing database connection...")
    
    try:
        from database import NewsDatabase
        db = NewsDatabase()
        count = db.get_article_count()
        print(f"âœ… Database connection successful - {count} articles in database")
        return True
    except Exception as e:
        print(f"âŒ Database connection failed: {e}")
        print("ğŸ’¡ Check your MONGO_URI and network connection")
        return False

def test_scraper_basic():
    """Test basic scraper functionality (without actually scraping)"""
    print("\nğŸ” Testing scraper components...")
    
    try:
        from scraper import AINewsScaper
        scraper = AINewsScaper()
        
        # Test AI tool classification
        test_text = "OpenAI releases new ChatGPT model with improved language understanding"
        tool_type = scraper.classify_ai_tool_type(test_text)
        
        if tool_type:
            print(f"âœ… AI classification working - detected: {tool_type}")
            return True
        else:
            print("âŒ AI classification failed")
            return False
            
    except Exception as e:
        print(f"âŒ Scraper test failed: {e}")
        return False

def run_all_tests():
    """Run all tests and provide summary"""
    print("ğŸ¤– AI News Collector - Setup Test\n")
    
    tests = [
        ("Package Imports", test_imports),
        ("Environment Variables", test_environment),
        ("Database Connection", test_database_connection),
        ("Scraper Components", test_scraper_basic)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} failed with error: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "="*50)
    print("ğŸ“‹ TEST SUMMARY")
    print("="*50)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} - {test_name}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š Results: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("ğŸ‰ All tests passed! Your setup is ready.")
        print("\nğŸš€ Next steps:")
        print("   1. Run the scraper: python scraper.py")
        print("   2. Start the dashboard: streamlit run dashboard.py")
    else:
        print("âš ï¸  Some tests failed. Please fix the issues above.")
    
    return passed == len(results)

if __name__ == "__main__":
    run_all_tests() 